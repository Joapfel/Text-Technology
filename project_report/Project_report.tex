\documentclass[bibliography=totoc]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman, english]{babel}

\usepackage{hyperref}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{tikz-qtree-compat}
\usepackage{graphicx} 

\title{Eberhard Karls\\ Universität\\ Tübingen \vspace{1em}\\Frequency effects data analysis}
\author{Instructor: Dr. Ching-Chu Sun\\Student: Johannes Krämer 3977732}
\date{2017}

\begin{document}
\maketitle

\section{Elasticsearch Project}
Here comes our motivation why we picked Elasticsearch.
We will also describe that our approach was a heterogenious which will be reflected in the different sections. 
Which projects were worked on: Ufo, Beer, Political etc

\section{The ELK Stack}
Very short introduction into the ELK Stack. What does it stand for, and a high level explenation of the single tools.
Usage possibilities: local, as a service (cloud), in own data centers.

\subsection{Logstash}
As already mentioned Logstash can be seen as a preprocessing tool.
It prepares the data and sends the transformed data to the specified 
Elasticsearch instance.
It is able to recieve data from all kinds of sources in all kinds of formats like from Databases, HTTP-Streams, 
CSV-files, JSON-files, Logfiles etc.
\\
With the help of filters the data is parsed and transformed into a
desired format. That way it is possible to achieve more structure, consistancy,
readability etc.
\\
In the next paragraph the configuration file which was used for the
Ufo-sighting project will be explained (see referenced Image).
\\
The Ufo-sightings are stored in a csv file. Hence the configuration file 
contains definitions specific for a csv file.
\\
In line 0-5 the input is defined (the csv file). Once dynamically read 
from the standart input \footnote{means the path to the file needs to be passed as an argument when running logstash} (stdin),
and once with a hardcoded path to the csv file.
\\
From line 7-31 different filters are defined which deal with the content 
of the csv file:
\\
The csv filter (line 8-12) specifies the structure of the original csv file.
In this case the csv file is comma seperated (could be tab seperated aswell),
has the following columns (see image) and the header is skipped because
it is not part of the actual data.
\\
The mutate filters (line 13-27) actually applied some changes to the data:
In line 14 we a new field \textit{location} is constructed out of the 
existing \textit{latitude} and \textit{longitude} fields. The reason for 
that is, that in order to represent geo-coordinates Elasticsearch requires geographical data to be stored as a tuple
in one field instead of beeing stored in seperate fields.
\\
On line 15 a couple of fields are removed, which where decided to be not relevant
for the data analysis anymore. For example geographical information is now 
stored in the \textit{location} field, hence the original \textit{lalitude}- and \textit{longitude}-fields
are not needed anymore.
\\
The \textit{gsup} mutation (line 16-23) is a regular expression based string subsitution.
The first column defines the source field, the second column defines the regular expression
and the third field is the replacement string.
\\
In this case the language codes are transformed into the original country names
just for the sake of readability.
\\
The last subsitution is a little hack to deal with a specificity of the Ufo-dataset.
In the Ufo-dataset midnight was encoded as 24:00. But Elasticsearch's time format goes from
00:00:00-23:59:59. Although it is more common to represent midnight as 00:00, sometimes
24:00 is used to represent the end of the day and 00:00 the beginning of the day.
Since the original midnight encoding was 24:00 (end of day) it was decided to 
remove one minute instead of adding one, such that the time still represents the end 
of a day instead of the beginning of the next day.
\\
The date filter (line 28-30) deals with different date representations.
Elasticsearch requires a consistent date-time representation to be automatically
recognized as such. Therefore all possible date combinations\footnote{A day represented with one digit. A day represented as two digits. A month represented as one digit. A month represented as two digits.} must be 
given to the date filter.
The first entry is the column name of the date field followed by the 
different possible combinations of date representation.
\\
\\
From line 32-42 the output is defined. This means where the transformed data is 
sent to:
In line 33 debugging information is sent to the standart output (stdout) - e.g. the console.
From line 34-41 the Elasticsearch target instance is defined together with the 
index the data should be stored at. In this case (line 36) Elasticsearch
runs locally (localhost) and the index (line 37) is called \textit{ufoindex}.
\\
As mentioned earlier Elasticsearch can be used as a service in the cloud. In line 
35 a Elasticsearch target instance can be seen which was hosted on Amazons AWS Cloud-system\footnote{A free trial was used in order to have all the material for the presentation ready in one place. The trial (and hence the url) is not active anymore.}.

\subsection{Elasticsearch}
Core technology. Here we should have King's queries and the respective explenation/discussion.
If someone else has queries to show put them here aswell.

\subsection{Kibana}
We can just put Dashbord-Screenshots here and explain what the visualization means.
Also mention that the Dashboards are interactive. Powerfull for descriptive statistics.

\section{Conclusion}
Fast, Powerfull bla bla bla...

\section{Personal Reflection}
Here we can put our personal opinions and say what we liked most.

\end{document}